{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database|Montage|Sample rate|Channels|Classes|Subjects\n",
    "-|-|-|-|-|-\n",
    "GIGA |Standard 1005 |512 Hz |64 |2 |52\n",
    "BCI2a |Standard 1020 |250 Hz |22 |4 |9\n",
    "HighGamma |Standard 1005 |500 Hz |133 |4 |14\n",
    "BCI illiteracy - MI |Standard 1020 |1000 Hz |62 |2 |54\n",
    "BCI illiteracy - SSVEP |Standard 1020 |1000 Hz |62 |4 |54\n",
    "BCI illiteracy - ERP |Standard 1020 |1000 Hz |62 |2 |54\n",
    "Physionet - MM/MI |Standard 1005 |160 Hz |64 |8 |109\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paradigms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GIGA \n",
    "[http://dx.doi.org/10.5524/100295](http://dx.doi.org/10.5524/100295)\n",
    "\n",
    "Subjects sat in a chair with armrests and watched a monitor. At the beginning of each trial, the monitor showed a black screen with a fixation cross for 2 seconds; the subject was then ready to perform hand movements (once the black screen gave a ready sign to the subject). One of two instructions (“left hand” or “right hand”) appeared randomly on\n",
    "the screen for 3 seconds, and subjects were asked to move the appropriate hand depending on the instruction given. After the movement, when the blank screen reappeared, the subject was given a break for a random 4.1 to 4.8 seconds. Five or six runs were performed during the MI experiment. After each run, we calculated the classification accuracy over one run and gave the subject feedback to increase motivation. Between each run, a maximum 4-minute break was given depending on the subject’s demands.\n",
    "\n",
    "![GIGA](images/giga.png)\n",
    "\n",
    ">Cho, H., Ahn, M., & Ahn, S. (2017). Supporting data for “EEG datasets for motor imagery brain computer interface.”. GigaScience Database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BCI2a\n",
    "[http://www.bbci.de/competition/iv/#datasets](http://www.bbci.de/competition/iv/#datasets)\n",
    "\n",
    "This data set consists of EEG data from 9 subjects. The cue-based BCI paradigm consisted of four different motor imagery tasks, namely the imagination of movement of the left hand (class 1), right hand (class 2), both feet (class  3), and tongue (class  4). Two sessions on different days were recorded for each subject. Each session is comprised of 6 runs separated byshort breaks. One run consists of 48 trials (12 for each of the four possible classes), yielding a total of 288 trials per session. At the beginning of each session, a recording of approximately 5 minutes was performed to estimate the EOG influence. The subjects were sitting in a comfortable armchair in front of a computer screen.  At the beginning of a trial (t=0 s), a fixation cross appeared on the black screen. In addition, a short acoustic warning tone was presented. After two seconds (t=2 s), a cue in the form of an arrow pointing either to the left, right, down or up (corresponding to one of the four classes left hand, right hand, foot or tongue) appeared and stayed on the screen for 1.25 s. This prompted the subjects to perform the desired motor imagerytask. No feedback was provided. The subjects were ask to carry out the motor imagery task until the fixation cross disappeared from the screen att=6 s. A short break followed where the screen was black again.\n",
    "\n",
    "![BCI2a](images/bci2a.png)\n",
    "\n",
    ">Brunner, C., Leeb, R., Müller-Putz, G., Schlögl, A., & Pfurtscheller, G. (2008). BCI Competition 2008–Graz data set A. Institute for Knowledge Discovery (Laboratory of Brain-Computer Interfaces), Graz University of Technology, 16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HighGamma\n",
    "\n",
    "[http://dx.doi.org/10.1002/hbm.23730](http://dx.doi.org/10.1002/hbm.23730)  \n",
    "![HighGamma](images/highgamma.png)\n",
    "\n",
    ">Schirrmeister, R. T., Springenberg, J. T., Fiederer, L. D. J., Glasstetter, M., Eggensperger, K., Tangermann, M., ... & Ball, T. (2017). Deep learning with convolutional neural networks for EEG decoding and visualization. Human brain mapping, 38(11), 5391-5420."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BCI illiteracy\n",
    "[https://doi.org/10.1093/gigascience/giz002](https://doi.org/10.1093/gigascience/giz002)\n",
    "\n",
    "#### MI\n",
    "For all blocks, the first 3 s of each trial began with a black fixation cross that appeared at the center of the monitor to prepare subjects for the MI task. Afterwards, the subject performed the imagery task of grasping with the appropriate hand for 4 s when the right or left arrow appeared as a visual cue. After each task, the screen remained blank for 6 s (± 1.5 s). The experiment consisted of training and test phases; each phase had 100 trials with balanced right and left hand imagery tasks.\n",
    "\n",
    "![BCIilliteracy-MI](images/illiteracy_mi.png)\n",
    "\n",
    "#### ERP\n",
    "The interface layout of the speller followed the typical design of a row-column speller. The six rows and six columns were configured with 36 symbols (A to Z, 1 to 9, and \\_). Each symbol was presented equally spaced. To enhance the signal quality, two additional settings were incorporated into the original row-column speller design, namely, random-set presentation and face stimuli. These additional settings help to elicit stronger ERP responses by minimizing adjacency distraction errors and by presenting a familiar face image. The stimulus-time interval was set to 80 ms, and the inter-stimulus interval (ISI) to 135 ms. A single iteration of stimulus presentation in all rows and columns was considered a sequence. Therefore, one sequence consisted of 12 stimulus flashes. A maximum of five sequences (i.e., 60 flashes) was allotted without prolonged inter-sequence intervals for each target character. After the end of five sequences, 4.5 s were given to the user for identifying, locating, and gazing at the next target character. The participant was instructed to attend to the target symbol by counting the number of times each target character had been flashed.\n",
    "\n",
    "![BCIilliteracy-ERP](images/illiteracy_erp.png)\n",
    "\n",
    "#### SSVEP\n",
    "Four target SSVEP stimuli were designed to flicker at 5.45, 6.67, 8.57, and 12 Hz and were presented in four positions (down, right, left, and up, respectively) on a monitor. The designed paradigm followed the conventional types of SSVEP-based BCI systems that require four-direction movements. Participants were asked to fixate the center of a black screen and then to gaze in the direction where the target stimulus was highlighted in a different color. Each SSVEP stimulus was presented for 4 s with an ISI of 6 s. Each target frequency was presented 25 times. Therefore, the corrected EEG data had 100 trials (4 classes × 25 trials) in the offline training phase and another 100 trials in the online test phase. Visual feedback was presented in the test phase; the estimated target frequency was highlighted for 1 s with a red border at the end of each trial.\n",
    "\n",
    "![BCIilliteracy-SSVEP](images/illiteracy_ssvep.png)\n",
    "\n",
    ">Lee, M. H., Kwon, O. Y., Kim, Y. J., Kim, H. K., Lee, Y. E., Williamson, J., ... & Lee, S. W. (2019). EEG dataset and OpenBMI toolbox for three BCI paradigms: an investigation into BCI illiteracy. GigaScience, 8(5), giz002."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physionet \n",
    "\n",
    "[https://doi.org/10.13026/C28G6P](https://doi.org/10.13026/C28G6P)  \n",
    "\n",
    "\n",
    "Subjects performed different motor/imagery tasks while 64-channel EEG were recorded using the BCI2000 system (http://www.bci2000.org). Each subject performed 14 experimental runs: two one-minute baseline runs (one with eyes open, one with eyes closed), and three two-minute runs of each of the four following tasks:\n",
    "\n",
    "  1. A target appears on either the left or the right side of the screen. The subject opens and closes the corresponding fist until the target disappears. Then the subject relaxes.\n",
    "  1. A target appears on either the left or the right side of the screen. The subject imagines opening and closing the corresponding fist until the target disappears. Then the subject relaxes.\n",
    "  1. A target appears on either the top or the bottom of the screen. The subject opens and closes either both fists (if the target is on top) or both feet (if the target is on the bottom) until the target disappears. Then the subject relaxes.\n",
    "  1. A target appears on either the top or the bottom of the screen. The subject imagines opening and closing either both fists (if the target is on top) or both feet (if the target is on the bottom) until the target disappears. Then the subject relaxes.\n",
    "\n",
    "![Physionet](images/physionet.png)\n",
    "\n",
    ">Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals. Circulation [Online]. 101 (23), pp. e215–e220."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
